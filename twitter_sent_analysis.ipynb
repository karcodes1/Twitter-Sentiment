{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bba4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import time\n",
    "import datetime as dt\n",
    "import re\n",
    "import pandas_datareader as pdr\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "# Pretty print all cell's output and not just the last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a08a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the scraped twitter data file after using twitter-scrape.py\n",
    "header_list = ['author id', 'created_at', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet','ticker_searched']\n",
    "twitter_data = pd.read_csv('./twitter_data_final.csv',\n",
    "                   names = header_list)\n",
    "twitter_data['date'] = pd.to_datetime(twitter_data['created_at'], infer_datetime_format = True)\n",
    "twitter_data = twitter_data.set_index(['date', 'ticker_searched'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load S&P500 firms for the time peroid\n",
    "START_DATE = '2011-03-01'\n",
    "END_DATE = '2012-03-01'\n",
    "\n",
    "SP500 = pd.read_csv('./S&P500.txt')\n",
    "SP500['dtdate'] = pd.to_datetime(SP500['date'], format = \"%Y-%m-%d\")\n",
    "SP500 = SP500.loc[(SP500['dtdate'] >= START_DATE) & (SP500['dtdate'] <= END_DATE)].copy()\n",
    "tickers = []\n",
    "\n",
    "#since firms have the potential to enter and exit the S&P500 several times a year we want to make sure that they are \n",
    "#all incldued\n",
    "i = SP500.index[SP500['dtdate'] == START_DATE][0]\n",
    "while i < SP500.index[SP500['dtdate'] == END_DATE][0]:\n",
    "    current_tickers = (SP500.at[i,'tickers'])\n",
    "    current_tickers_arr = current_tickers.split(',')\n",
    "    for tickers_ in current_tickers_arr:\n",
    "        tickers.append(str.lower('$'+tickers_))\n",
    "    i = i + 1\n",
    "    \n",
    "all_tickers = set(tickers)\n",
    "tickers = list(all_tickers)\n",
    "tickers.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb14a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function counts the number of times a ticker is present in the body of a tweet\n",
    "#for example if we have the tweet with the following context:\n",
    "#\"strong day for $AAPL, $AMZN, $MSFT\"\n",
    "#the function would return 3 and save it as 'ticker_count'\n",
    "\n",
    "def ticker_count(x):\n",
    "    pattern = re.compile('\\W')\n",
    "    string = re.sub(r'[^A-Za-z0-9 $]+', '', x)\n",
    "    words = string.split()\n",
    "    tickers_count = 0\n",
    "    for words_ in words:\n",
    "        if words_.lower() in tickers:\n",
    "            tickers_count += 1\n",
    "    return tickers_count\n",
    "twitter_data['ticker_count'] = twitter_data['tweet'].apply(ticker_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are now able to remove all data that contains more than 1 ticker\n",
    "twitter_data = twitter_data.loc[twitter_data['ticker_count'] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cb618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function converts the 'created_at' variable from an object to a date_time\n",
    "#for whatever reason the twitter API returns the date in two different formats -- this will work for both\n",
    "\n",
    "def date_fix(x):\n",
    "    if len(x) == 25:\n",
    "        x = x[0:19]\n",
    "        x = pd.to_datetime(x, format = '%Y-%m-%d %H:%M:%S')\n",
    "    elif len(x) == 19:\n",
    "        x = x \n",
    "        x = pd.to_datetime(x, format = '%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        x = np.nan\n",
    "    return x\n",
    "twitter_data['fixed_dt'] = twitter_data['created_at'].apply(date_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f10271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function assigns the 'trading_day' for the tweet\n",
    "#1. comments made after hours are considered for the next trading day\n",
    "#2. comments made on Sunday are considered for the next trading day (monday)\n",
    "#3. comments made on Saturday are considered for the next trading day (monday)\n",
    "\n",
    "def trading_day(x):\n",
    "    if x.hour > 16:\n",
    "        x = x + pd.DateOffset(1)\n",
    "    if x.weekday() == 6:\n",
    "        x = x + pd.DateOffset(1)\n",
    "    if x.weekday() == 5:\n",
    "        x = x + pd.DateOffset(2)\n",
    "    return x\n",
    "twitter_data['trading_day'] = twitter_data['fixed_dt'].apply(trading_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we load the harvard 4 sentiment dictonary\n",
    "#we make two dictionarys \n",
    "#1. words that are attributed with positive sentiment\n",
    "#2. words that are attributed with negative sentiment\n",
    "\n",
    "dictionary = pd.read_csv('./dictionary.csv', low_memory=False)\n",
    "dictionary['Entry'] = dictionary['Entry'].str.lower()\n",
    "dictionary['Entry'] = dictionary.Entry.str.replace('[^a-zA-Z]',r'',regex=True)\n",
    "dictionary = dictionary.loc[dictionary['Source'] == 'H4'].copy()\n",
    "pos = dictionary.loc[dictionary['Positiv'] == 'Positiv'][['Positiv', 'Entry']]\n",
    "pos = pos.set_index(pos['Entry'])\n",
    "pos = pos.drop_duplicates()\n",
    "pos_words = pos.to_dict('index')\n",
    "neg = dictionary.loc[dictionary['Negativ'] == 'Negativ'][['Negativ', 'Entry']]\n",
    "neg = neg.set_index(neg['Entry'])\n",
    "neg = neg.drop_duplicates()\n",
    "neg_words = neg.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we count the number of pos/neg sentiment words that exist in each tweet\n",
    "\n",
    "def pos_sentiment(x):\n",
    "    words = x.split()\n",
    "    pos_count = 0\n",
    "    for words_ in words:\n",
    "        if words_.lower() in pos_words:\n",
    "            pos_count += 1\n",
    "    return pos_count\n",
    "def neg_sentiment(x):\n",
    "    words = x.split()\n",
    "    neg_count = 0\n",
    "    for words_ in words:\n",
    "        if words_.lower() in neg_words:\n",
    "            neg_count += 1\n",
    "    return neg_count\n",
    "twitter_data['pos_sentiment'] = twitter_data['tweet'].apply(pos_sentiment)\n",
    "twitter_data['neg_sentiment'] = twitter_data['tweet'].apply(neg_sentiment)\n",
    "\n",
    "#we also need the number of words in each tweet\n",
    "twitter_data['words_count'] = twitter_data['tweet'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5388ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load market data and make the dates match the format of twitter_data['trading_day']\n",
    "market_data = pd.read_csv('./hsvtcapxk3srzvtd.csv', low_memory = False)\n",
    "market_data['datadate'] = pd.to_datetime(market_data['date'], format = \"%Y/%m/%d\").copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b8a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the twitter_data and market_data by date and firm\n",
    "\n",
    "market_data['just_date'] = market_data['datadate'].dt.date\n",
    "twitter_data['trading_ddate'] = twitter_data['trading_day'].dt.date\n",
    "merge1 = pd.merge(twitter_data, market_data, \n",
    "                                how = 'left',\n",
    "                                left_on = ['trading_ddate', 'ticker_searched'],\n",
    "                                right_on = ['just_date', 'TICKER']\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733945b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove data that we will not be using \n",
    "merge1 = merge1[['TICKER', 'pos_sentiment', 'neg_sentiment', 'trading_ddate', 'RET', 'words_count']].copy()\n",
    "merge1 = merge1.set_index(['trading_ddate', 'TICKER']).copy()\n",
    "merge1 = merge1.sort_index(axis=0)\n",
    "merge1 = merge1.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8aa166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of pos/neg sentiment on a per day/per firm basis\n",
    "pos_sentiment_daily = merge1.groupby([merge1.index.get_level_values(0), merge1.index.get_level_values(1), merge1['RET']])['pos_sentiment'].sum().to_frame()\n",
    "neg_sentiment_daily = merge1.groupby([merge1.index.get_level_values(0), merge1.index.get_level_values(1), merge1['RET']])['neg_sentiment'].sum().to_frame()\n",
    "\n",
    "#determine the number of total words and tweets per day/per firm\n",
    "daily_word_count = merge1.groupby([merge1.index.get_level_values(0), merge1.index.get_level_values(1), merge1['RET']])['words_count'].sum().to_frame()\n",
    "tweet_count = merge1.groupby([merge1.index.get_level_values(0), merge1.index.get_level_values(1), merge1['RET']])['neg_sentiment'].count().to_frame()\n",
    "total_tweets = merge1.groupby([merge1.index])\n",
    "\n",
    "#merge dataframes\n",
    "sentiment2 = pos_sentiment_daily.merge(neg_sentiment_daily, left_index = True, right_index = True)\n",
    "sentiment1 = sentiment2.merge(daily_word_count, left_index = True, right_index = True)\n",
    "\n",
    "tweet_count['count'] = tweet_count['neg_sentiment']\n",
    "tweet_count = tweet_count.drop(columns = 'neg_sentiment')\n",
    "sentiment = sentiment1.merge(tweet_count, left_index = True, right_index = True) \n",
    "sentiment['total_sentiment'] = sentiment['pos_sentiment'] + sentiment['neg_sentiment']\n",
    "\n",
    "#calcualte the emotional valence variables\n",
    "sentiment['neg1'] = (sentiment['neg_sentiment']/sentiment['words_count'])\n",
    "sentiment['pos1'] = ((sentiment['pos_sentiment'] - sentiment['neg_sentiment']) / (sentiment['total_sentiment'])).fillna(0)\n",
    "sentiment['pos2'] = np.log10( (1 + sentiment['pos_sentiment']) / (1 + sentiment['neg_sentiment']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the CRSP/Compustat data\n",
    "comp_data = pd.read_csv('./comp_data.csv',\n",
    "                           low_memory = False)\n",
    "comp_data2 = comp_data.loc[comp_data['exchg'] == 11]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset sentiment index for merge\n",
    "sentiment = sentiment.reset_index()\n",
    "\n",
    "#merge sentiment and comp_data\n",
    "merge2 = pd.merge(sentiment, comp_data, \n",
    "                                how = 'left',\n",
    "                                left_on = ['TICKER'],\n",
    "                                right_on = ['tic']\n",
    "                 )\n",
    "\n",
    "#the trading_ddate variable will occasionally revert to an object data-type, we need it to be datetime data-type\n",
    "merge2['trading_ddate'] = pd.to_datetime(merge2['trading_ddate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1715a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the book-to-market for each firm\n",
    "merge2['book_to_market'] = (merge2['at'] - merge2['lt'])/(merge2['mkvalt'])\n",
    "\n",
    "#calculate the book-to-market quartiles for constructing the Fama French portfolio\n",
    "mkt_value_median = comp_data2['mkvalt'].median()\n",
    "comp_data2 = comp_data2.dropna().copy()\n",
    "comp_data2['book_to_market'] = (comp_data2['at'] - comp_data2['lt'])/(comp_data2['mkvalt'])\n",
    "book_to_market_quartiles = comp_data2['book_to_market'].quantile(q=[.3,.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fb929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Fama French data\n",
    "kf_data = pd.read_csv('./6_Portfolios_2x3_daily.csv',\n",
    "                      low_memory = False,\n",
    "                     nrows=25010)\n",
    "#fix column name\n",
    "kf_data.rename(columns = {'Unnamed: 0':'date'}, inplace = True)\n",
    "kf_data = kf_data.dropna().copy()\n",
    "\n",
    "#make sure date matches format from merge2\n",
    "kf_data['dtdate'] = pd.to_datetime(kf_data['date'], format = '%Y%m%d')\n",
    "\n",
    "#convert all percentages to fractions\n",
    "kf_data['SMALL LoBM'] = kf_data['SMALL LoBM']/100\n",
    "kf_data['ME1 BM2'] = kf_data['ME1 BM2']/100\n",
    "kf_data['SMALL HiBM'] = kf_data['SMALL HiBM']/100\n",
    "kf_data['BIG LoBM'] = kf_data['BIG LoBM']/100\n",
    "kf_data['ME2 BM2'] = kf_data['ME2 BM2']/100\n",
    "kf_data['BIG HiBM'] = kf_data['BIG HiBM']/100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe2fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with Fama French data\n",
    "merge3 = pd.merge(merge2, kf_data, \n",
    "                                how = 'left',\n",
    "                                left_on = ['trading_ddate'],\n",
    "                                right_on = ['dtdate']\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3531dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all data that we won't use\n",
    "final_data = merge3[['trading_ddate', 'TICKER', 'RET','total_sentiment', 'neg1', 'pos1', 'pos2', 'book_to_market', 'mkvalt', 'SMALL LoBM', 'ME1 BM2', 'SMALL HiBM', 'BIG LoBM', 'ME2 BM2', 'BIG HiBM', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36438c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRSP has different error codes for dates without returns we need to remove this data and convert to float.\n",
    "final_data.loc[final_data['RET'] == 'C', 'RET'] = None\n",
    "final_data.loc[final_data['RET'] == 'A', 'RET'] = None\n",
    "final_data.loc[final_data['RET'] == 'B', 'RET'] = None\n",
    "final_data.loc[final_data['RET'] == 'D', 'RET'] = None\n",
    "final_data.loc[final_data['RET'] == 'E', 'RET'] = None\n",
    "final_data = final_data.dropna().copy()\n",
    "final_data['RET'] = final_data['RET'].astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693adcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#organize the firms based on their book-to-market and size (6 FF portfolios)\n",
    "#calculate the abnormal return of each firm\n",
    "#this is done for all 6 FF portfolios\n",
    "df_SMALL_LOBM = final_data.loc[(final_data['book_to_market'] < book_to_market_quartiles.iloc[0]) & (final_data['mkvalt'] < mkt_value_median)]\n",
    "df_SMALL_LOBM['abnormal_ret'] = df_SMALL_LOBM['RET'] - df_SMALL_LOBM['SMALL LoBM']\n",
    "\n",
    "df_SMALL_BM2 = final_data.loc[(final_data['book_to_market'] < book_to_market_quartiles.iloc[1]) & (final_data['book_to_market'] > book_to_market_quartiles.iloc[0]) & (final_data['mkvalt'] < mkt_value_median)]\n",
    "df_SMALL_BM2['abnormal_ret'] = df_SMALL_BM2['RET'] - df_SMALL_BM2['ME1 BM2']\n",
    "\n",
    "df_SMALL_HiBM = final_data.loc[(final_data['book_to_market'] > book_to_market_quartiles.iloc[1]) & (final_data['mkvalt'] < mkt_value_median)]\n",
    "df_SMALL_HiBM['abnormal_ret'] = df_SMALL_HiBM['RET'] - df_SMALL_HiBM['SMALL HiBM']\n",
    "\n",
    "df_BIG_LOBM = final_data.loc[(final_data['book_to_market'] < book_to_market_quartiles.iloc[0]) & (final_data['mkvalt'] > mkt_value_median)]\n",
    "df_BIG_LOBM['abnormal_ret'] = df_BIG_LOBM['RET'] - df_BIG_LOBM['BIG LoBM']\n",
    "\n",
    "df_BIG_BM2 = final_data.loc[(final_data['book_to_market'] < book_to_market_quartiles.iloc[1]) & (final_data['book_to_market'] > book_to_market_quartiles.iloc[0]) & (final_data['mkvalt'] > mkt_value_median)]\n",
    "df_BIG_BM2['abnormal_ret'] = df_BIG_BM2['RET'] - df_BIG_BM2['ME2 BM2']\n",
    "\n",
    "df_BIG_HiBM = final_data.loc[(final_data['book_to_market'] > book_to_market_quartiles.iloc[1]) & (final_data['mkvalt'] > mkt_value_median)]\n",
    "df_BIG_HiBM['abnormal_ret'] = df_BIG_HiBM['RET'] - df_BIG_HiBM['BIG HiBM']\n",
    "\n",
    "#combine all the portfolios\n",
    "final_df = pd.concat([df_SMALL_LOBM, df_SMALL_BM2, df_SMALL_HiBM, df_BIG_LOBM, df_BIG_BM2, df_BIG_HiBM])\n",
    "\n",
    "#index by date\n",
    "final_df.set_index(['trading_ddate'])\n",
    "final_df.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#having percentage returns makes calculating cum returns easier\n",
    "final_df['abnormal_ret_plus_1'] = final_df['abnormal_ret'] + 1\n",
    "\n",
    "#calculate the previous dat reuturn for Control 1 var\n",
    "final_df['abnormal_ret_1d_prior'] = final_df.groupby('TICKER')['abnormal_ret'].shift(1)\n",
    "\n",
    "#calculate the cumulative returns for each ticker\n",
    "final_df['cum_prod'] = final_df.groupby('TICKER')['abnormal_ret_plus_1'].cumprod()\n",
    "\n",
    "#calculate the next 10 day returns\n",
    "final_df['abnormal_ret_next_10days'] = final_df.groupby('TICKER')['cum_prod'].shift(-9) / final_df['cum_prod']\n",
    "final_df['abnormal_ret_next_10days'] = np.power(final_df['abnormal_ret_next_10days'], 1/10) - 1\n",
    "\n",
    "#calculate the returns for the previous month without the previous dat for Control 2 var\n",
    "final_df['abnormal_ret_30d_to_2d_prior'] = final_df.groupby('TICKER')['cum_prod'].shift(1) / final_df.groupby('TICKER')['cum_prod'].shift(29)\n",
    "final_df['abnormal_ret_30d_to_2d_prior'] = final_df['abnormal_ret_30d_to_2d_prior'] - 1\n",
    "\n",
    "#calculate previous dat returns\n",
    "final_df['previous_day_ret'] = (1/final_df.groupby('TICKER')['cum_prod'].shift(1)) * final_df['cum_prod']\n",
    "final_df['previous_day_ret'] = final_df.groupby('TICKER')['previous_day_ret'].shift(1)\n",
    "final_df['previous_day_ret'] = final_df['previous_day_ret'] -1\n",
    "\n",
    "#calculate next dat returns\n",
    "final_df['abnormal_ret_next_day'] = final_df.groupby('TICKER')['abnormal_ret'].shift(-1)\n",
    "\n",
    "#since we have rolling returns we will not have data for all time peroids, so we need to drop rows with empty values\n",
    "final_df = final_df.dropna()\n",
    "\n",
    "#drop any duplicates with same ticker and trading date\n",
    "final_df = final_df.drop_duplicates(subset = ['TICKER', 'trading_ddate'], keep = 'first').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c91abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can now run our OLS regression\n",
    "\n",
    "x1 = ['pos1', 'abnormal_ret_30d_to_2d_prior', 'previous_day_ret']\n",
    "x2 = ['pos2', 'abnormal_ret_30d_to_2d_prior', 'previous_day_ret']\n",
    "x3 = ['neg1', 'abnormal_ret_30d_to_2d_prior', 'previous_day_ret']\n",
    "y1 = 'abnormal_ret'\n",
    "y2 = 'abnormal_ret_next_day'\n",
    "y3 = 'abnormal_ret_next_10days'\n",
    "\n",
    "y = final_df[y1]\n",
    "x = final_df[x1]\n",
    "\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "res = sm.OLS(y,x).fit()\n",
    "res\n",
    "\n",
    "#print(res.summary())\n",
    "#print(res.rsquared_adj)\n",
    "#print(res.params)\n",
    "#print(res.pvalues)\n",
    "#print(res.bse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
